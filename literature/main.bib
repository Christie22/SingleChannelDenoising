
@article{kolbaek_single-microphone_2018,
	title = {Single-{Microphone} {Speech} {Enhancement} and {Separation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1808.10620},
	abstract = {The cocktail party problem comprises the challenging task of understanding a speech signal in a complex acoustic environment, where multiple speakers and background noise signals simultaneously interfere with the speech signal of interest. A signal processing algorithm that can effectively increase the speech intelligibility and quality of speech signals in such complicated acoustic situations is highly desirable. Especially for applications involving mobile communication devices and hearing assistive devices. Due to the re-emergence of machine learning techniques, today, known as deep learning, the challenges involved with such algorithms might be overcome. In this PhD thesis, we study and develop deep learning-based techniques for two sub-disciplines of the cocktail party problem: single-microphone speech enhancement and single-microphone multi-talker speech separation. Specifically, we conduct in-depth empirical analysis of the generalizability capability of modern deep learning-based single-microphone speech enhancement algorithms. We show that performance of such algorithms is closely linked to the training data, and good generalizability can be achieved with carefully designed training data. Furthermore, we propose uPIT, a deep learning-based algorithm for single-microphone speech separation and we report state-of-the-art results on a speaker-independent multi-talker speech separation task. Additionally, we show that uPIT works well for joint speech separation and enhancement without explicit prior knowledge about the noise type or number of speakers. Finally, we show that deep learning-based speech enhancement algorithms designed to minimize the classical short-time spectral amplitude mean squared error leads to enhanced speech signals which are essentially optimal in terms of STOI, a state-of-the-art speech intelligibility estimator.},
	urldate = {2019-04-17},
	journal = {arXiv:1808.10620 [cs, eess]},
	author = {Kolbæk, Morten},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10620},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1808.10620 PDF:/Users/miccio/Zotero/storage/4245V375/Kolbæk - 2018 - Single-Microphone Speech Enhancement and Separatio.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/7GZRYR96/1808.html:text/html}
}

@article{wang_supervised_2017,
	title = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}: {An} {Overview}},
	shorttitle = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}},
	url = {http://arxiv.org/abs/1708.07524},
	abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
	urldate = {2019-04-17},
	journal = {arXiv:1708.07524 [cs]},
	author = {Wang, DeLiang and Chen, Jitong},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07524},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/PLIFEXMS/1708.html:text/html;Wang and Chen - 2017 - Supervised Speech Separation Based on Deep Learnin.pdf:/Users/miccio/Zotero/storage/ADSTPLFF/Wang and Chen - 2017 - Supervised Speech Separation Based on Deep Learnin.pdf:application/pdf}
}

@misc{brian_mcfee_librosa/librosa:_2019,
	title = {librosa/librosa: 0.6.3},
	shorttitle = {librosa/librosa},
	url = {https://zenodo.org/record/2564164},
	abstract = {This release contains a few minor bugfixes and many improvements to documentation and usability.},
	urldate = {2019-04-17},
	publisher = {Zenodo},
	author = {Brian McFee and Matt McVicar and Stefan Balke and Vincent Lostanlen and Carl Thomé and Colin Raffel and Dana Lee and Kyungyun Lee and Oriol Nieto and Frank Zalkow and Dan Ellis and Eric Battenberg and Ryuichi Yamamoto and Josh Moore and Ziyao Wei and Rachel Bittner and Keunwoo Choi and nullmightybofo and Pius Friesch and Fabian-Robert Stöter and Thassilo and Matt Vollrath and Siddhartha Kumar Golu and nehz and Simon Waloschek and Seth and Rimvydas Naktinis and Douglas Repetto and Curtis "Fjord" Hawthorne and CJ Carr},
	month = feb,
	year = {2019},
	doi = {10.5281/zenodo.2564164},
	file = {Zenodo Snapshot:/Users/miccio/Zotero/storage/CQVYNR5R/2564164.html:text/html}
}

@article{fu_complex_2017,
	title = {Complex spectrogram enhancement by convolutional neural network with multi-metrics learning},
	url = {http://arxiv.org/abs/1704.08504},
	abstract = {This paper aims to address two issues existing in the current speech enhancement methods: 1) the difficulty of phase estimations; 2) a single objective function cannot consider multiple metrics simultaneously. To solve the first problem, we propose a novel convolutional neural network (CNN) model for complex spectrogram enhancement, namely estimating clean real and imaginary (RI) spectrograms from noisy ones. The reconstructed RI spectrograms are directly used to synthesize enhanced speech waveforms. In addition, since log-power spectrogram (LPS) can be represented as a function of RI spectrograms, its reconstruction is also considered as another target. Thus a unified objective function, which combines these two targets (reconstruction of RI spectrograms and LPS), is equivalent to simultaneously optimizing two commonly used objective metrics: segmental signal-to-noise ratio (SSNR) and logspectral distortion (LSD). Therefore, the learning process is called multi-metrics learning (MML). Experimental results confirm the effectiveness of the proposed CNN with RI spectrograms and MML in terms of improved standardized evaluation metrics on a speech enhancement task.},
	urldate = {2019-04-17},
	journal = {arXiv:1704.08504 [cs, stat]},
	author = {Fu, Szu-Wei and Hu, Ting-yao and Tsao, Yu and Lu, Xugang},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08504},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/CRTJWEPL/1704.html:text/html;Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:/Users/miccio/Zotero/storage/78N8IRC7/Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:application/pdf}
}

@article{roux_phasebook_2018,
	title = {Phasebook and {Friends}: {Leveraging} {Discrete} {Representations} for {Source} {Separation}},
	shorttitle = {Phasebook and {Friends}},
	url = {http://arxiv.org/abs/1810.01395},
	abstract = {Deep learning based speech enhancement and source separation systems have recently reached unprecedented levels of quality, to the point that performance is reaching a new ceiling. Most systems rely on estimating the magnitude of a target source by estimating a real-valued mask to be applied to a time-frequency representation of the mixture signal. A limiting factor in such approaches is a lack of phase estimation: the phase of the mixture is most often used when reconstructing the estimated time-domain signal. Here, we propose "magbook", "phasebook", and "combook", three new types of layers based on discrete representations that can be used to estimate complex time-frequency masks. Magbook layers extend classical sigmoidal units and a recently introduced convex softmax activation for mask-based magnitude estimation. Phasebook layers use a similar structure to give an estimate of the phase mask without suffering from phase wrapping issues. Combook layers are an alternative to the magbook-phasebook combination that directly estimate complex masks. We present various training and inference schemes involving these representations, and explain in particular how to include them in an end-to-end learning framework. We also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations. We evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus for single-channel speaker-independent speaker separation, matching the performance of state-of-the-art mask-based approaches without requiring additional phase reconstruction steps.},
	urldate = {2019-04-17},
	journal = {arXiv:1810.01395 [cs, eess, stat]},
	author = {Roux, Jonathan Le and Wichern, Gordon and Watanabe, Shinji and Sarroff, Andy and Hershey, John R.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01395},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv\:1810.01395 PDF:/Users/miccio/Zotero/storage/4KCJKJ4E/Roux et al. - 2018 - Phasebook and Friends Leveraging Discrete Represe.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/45NPB657/1810.html:text/html}
}

@article{zhen_psychoacoustically_2018,
	title = {On {Psychoacoustically} {Weighted} {Cost} {Functions} {Towards} {Resource}-{Efficient} {Deep} {Neural} {Networks} for {Speech} {Denoising}},
	url = {http://arxiv.org/abs/1801.09774},
	abstract = {We present a psychoacoustically enhanced cost function to balance network complexity and perceptual performance of deep neural networks for speech denoising. While training the network, we utilize perceptual weights added to the ordinary mean-squared error to emphasize contribution from frequency bins which are most audible while ignoring error from inaudible bins. To generate the weights, we employ psychoacoustic models to compute the global masking threshold from the clean speech spectra. We then evaluate the speech denoising performance of our perceptually guided neural network by using both objective and perceptual sound quality metrics, testing on various network structures ranging from shallow and narrow ones to deep and wide ones. The experimental results showcase our method as a valid approach for infusing perceptual significance to deep neural network operations. In particular, the more perceptually sensible enhancement in performance seen by simple neural network topologies proves that the proposed method can lead to resource-efficient speech denoising implementations in small devices without degrading the perceived signal fidelity.},
	urldate = {2019-04-17},
	journal = {arXiv:1801.09774 [cs, eess]},
	author = {Zhen, Kai and Sivaraman, Aswin and Sung, Jongmo and Kim, Minje},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.09774},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1801.09774 PDF:/Users/miccio/Zotero/storage/BBBF8AZM/Zhen et al. - 2018 - On Psychoacoustically Weighted Cost Functions Towa.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/SB2GTY87/1801.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-04-17},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.03167 PDF:/Users/miccio/Zotero/storage/5UQ44P38/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/9PVBPIRY/1502.html:text/html}
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2019-04-17},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Approximation, Completeness, Neural networks},
	pages = {303--314}
}

@inproceedings{pons_experimenting_2016,
	title = {Experimenting with musically motivated convolutional neural networks},
	doi = {10.1109/CBMI.2016.7500246},
	abstract = {A common criticism of deep learning relates to the difficulty in understanding the underlying relationships that the neural networks are learning, thus behaving like a black-box. In this article we explore various architectural choices of relevance for music signals classification tasks in order to start understanding what the chosen networks are learning. We first discuss how convolutional filters with different shapes can fit specific musical concepts and based on that we propose several musically motivated architectures. These architectures are then assessed by measuring the accuracy of the deep learning model in the prediction of various music classes using a known dataset of audio recordings of ballroom music. The classes in this dataset have a strong correlation with tempo, what allows assessing if the proposed architectures are learning frequency and/or time dependencies. Additionally, a black-box model is proposed as a baseline for comparison. With these experiments we have been able to understand what some deep learning based algorithms can learn from a particular set of data.},
	booktitle = {2016 14th {International} {Workshop} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	author = {Pons, J. and Lidy, T. and Serra, X.},
	month = jun,
	year = {2016},
	keywords = {audio recording dataset, audio signal processing, audio tempo, ballroom music, black-box model, Computer architecture, convolutional filters, deep learning, filtering theory, Instruments, learning (artificial intelligence), learning frequency dependencies, learning time dependencies, Machine learning, music, Music, music class prediction, music signal classification tasks, musical concepts, musically-motivated convolutional neural network architecture, neural net architecture, Neural networks, Shape, signal classification, Time-frequency analysis},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/miccio/Zotero/storage/LW7UHMMG/7500246.html:text/html;Pons et al. - 2016 - Experimenting with musically motivated convolution.pdf:/Users/miccio/Zotero/storage/CENBI2GI/Pons et al. - 2016 - Experimenting with musically motivated convolution.pdf:application/pdf}
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2019-04-17},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1803.01271 PDF:/Users/miccio/Zotero/storage/MNJ37Q67/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/H2Y2CBVI/1803.html:text/html}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-04-17},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1609.03499 PDF:/Users/miccio/Zotero/storage/REGCM3XP/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/4PX3AQDD/1609.html:text/html}
}

@inproceedings{marchi_novel_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional {LSTM} neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178320/},
	doi = {10.1109/ICASSP.2015.7178320},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Marchi, Erik and Vesperini, Fabio and Eyben, Florian and Squartini, Stefano and Schuller, Bjorn},
	month = apr,
	year = {2015},
	pages = {1996--2000},
	file = {Marchi et al. - 2015 - A novel approach for automatic acoustic novelty de.pdf:/Users/miccio/Zotero/storage/NHP466KK/Marchi et al. - 2015 - A novel approach for automatic acoustic novelty de.pdf:application/pdf}
}

@inproceedings{weninger_discriminatively_2014,
	address = {Atlanta, GA, USA},
	title = {Discriminatively trained recurrent neural networks for single-channel speech separation},
	isbn = {978-1-4799-7088-9},
	url = {http://ieeexplore.ieee.org/document/7032183/},
	doi = {10.1109/GlobalSIP.2014.7032183},
	abstract = {This paper describes an in-depth investigation of training criteria, network architectures and feature representations for regressionbased single-channel speech separation with deep neural networks (DNNs). We use a generic discriminative training criterion corresponding to optimal source reconstruction from time-frequency masks, and introduce its application to speech separation in a reduced feature space (Mel domain). A comparative evaluation of time-frequency mask estimation by DNNs, recurrent DNNs and non-negative matrix factorization on the 2nd CHiME Speech Separation and Recognition Challenge shows consistent improvements by discriminative training, whereas long short-term memory recurrent DNNs obtain the overall best results. Furthermore, our results conﬁrm the importance of ﬁne-tuning the feature representation for DNN training.},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2014 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing} ({GlobalSIP})},
	publisher = {IEEE},
	author = {Weninger, Felix and Hershey, John R. and Le Roux, Jonathan and Schuller, Bjorn},
	month = dec,
	year = {2014},
	pages = {577--581},
	file = {Weninger et al. - 2014 - Discriminatively trained recurrent neural networks.pdf:/Users/miccio/Zotero/storage/U62EJZ9J/Weninger et al. - 2014 - Discriminatively trained recurrent neural networks.pdf:application/pdf}
}

@article{hulser_supervisors:_nodate,
	title = {Supervisors: {Prof}. {Alois} {Sontacchi} ({IEM}), {Dr}. {Gerald} {Bauer} ({Harman}) {Straubing}, {March} 25, 2018},
	language = {en},
	author = {Hülser, Gabriel},
	pages = {78},
	file = {Hülser - Supervisors Prof. Alois Sontacchi (IEM), Dr. Gera.pdf:/Users/miccio/Zotero/storage/DB4QHXQG/Hülser - Supervisors Prof. Alois Sontacchi (IEM), Dr. Gera.pdf:application/pdf}
}

@inproceedings{erdogan_phase-sensitive_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178061/},
	doi = {10.1109/ICASSP.2015.7178061},
	abstract = {Separation of speech embedded in non-stationary interference is a challenging problem that has recently seen dramatic improvements using deep network-based methods. Previous work has shown that estimating a masking function to be applied to the noisy spectrum is a viable approach that can be improved by using a signalapproximation based objective function. Better modeling of dynamics through deep recurrent networks has also been shown to improve performance. Here we pursue both of these directions. We develop a phase-sensitive objective function based on the signal-to-noise ratio (SNR) of the reconstructed signal, and show that in experiments it yields uniformly better results in terms of signal-to-distortion ratio (SDR). We also investigate improvements to the modeling of dynamics, using bidirectional recurrent networks, as well as by incorporating speech recognition outputs in the form of alignment vectors concatenated with the spectral input features. Both methods yield further improvements, pointing to tighter integration of recognition with separation as a promising future direction.},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Erdogan, Hakan and Hershey, John R. and Watanabe, Shinji and Le Roux, Jonathan},
	month = apr,
	year = {2015},
	pages = {708--712},
	file = {Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:/Users/miccio/Zotero/storage/XS3WKTAN/Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:application/pdf}
}

@article{grais_single_2017,
	title = {Single {Channel} {Audio} {Source} {Separation} using {Convolutional} {Denoising} {Autoencoders}},
	url = {http://arxiv.org/abs/1703.08019},
	abstract = {Deep learning techniques have been used recently to tackle the audio source separation problem. In this work, we propose to use deep fully convolutional denoising autoencoders (CDAEs) for monaural audio source separation. We use as many CDAEs as the number of sources to be separated from the mixed signal. Each CDAE is trained to separate one source and treats the other sources as background noise. The main idea is to allow each CDAE to learn suitable spectral-temporal ﬁlters and features to its corresponding source. Our experimental results show that CDAEs perform source separation slightly better than the deep feedforward neural networks (FNNs) even with fewer parameters than FNNs.},
	language = {en},
	urldate = {2019-04-17},
	journal = {arXiv:1703.08019 [cs]},
	author = {Grais, Emad M. and Plumbley, Mark D.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.08019},
	keywords = {Computer Science - Sound, 68T01, H.5.5, I.2.6, I.4.3, I.5},
	file = {Grais and Plumbley - 2017 - Single Channel Audio Source Separation using Convo.pdf:/Users/miccio/Zotero/storage/G38N9294/Grais and Plumbley - 2017 - Single Channel Audio Source Separation using Convo.pdf:application/pdf}
}

@article{kayser_denoising_nodate,
	title = {Denoising {Convolutional} {Autoencoders} for {Noisy} {Speech} {Recognition}},
	abstract = {We propose the use of a deep denoising convolutional autoencoder to mitigate problems of noise in real-world automatic speech recognition. We propose an overall pipeline for denoising, experiment with a variety of network architectures and conﬁguration parameters, and report results on an intermediate reconstruction error metric. Our experiments show that optimal conﬁguration for convolutional denoising varies signiﬁcantly from that for standard CNN tasks such as image recognition. Further, the proposed method easily beats a simple baseline and anecdotally displays interesting behavior. A number of avenues for further research are discussed.},
	language = {en},
	author = {Kayser, Mike and Zhong, Victor},
	pages = {6},
	file = {Kayser and Zhong - Denoising Convolutional Autoencoders for Noisy Spe.pdf:/Users/miccio/Zotero/storage/JYM3DZH5/Kayser and Zhong - Denoising Convolutional Autoencoders for Noisy Spe.pdf:application/pdf}
}

@article{lu_speech_nodate,
	title = {Speech {Enhancement} {Based} on {Deep} {Denoising} {Autoencoder}},
	abstract = {We previously have applied deep autoencoder (DAE) for noise reduction and speech enhancement. However, the DAE was trained using only clean speech. In this study, by using noisyclean training pairs, we further introduce a denoising process in learning the DAE. In training the DAE, we still adopt greedy layer-wised pretraining plus ﬁne tuning strategy. In pretraining, each layer is trained as a one-hidden-layer neural autoencoder (AE) using noisy-clean speech pairs as input and output (or transformed noisy-clean speech pairs by preceding AEs). Fine tuning was done by stacking all AEs with pretrained parameters for initialization. The trained DAE is used as a ﬁlter for speech estimation when noisy speech is given. Speech enhancement experiments were done to examine the performance of the trained denoising DAE. Noise reduction, speech distortion, and perceptual evaluation of speech quality (PESQ) criteria are used in the performance evaluations. Experimental results show that adding depth of the DAE consistently increase the performance when a large training data set is given. In addition, compared with a minimum mean square error based speech enhancement algorithm, our proposed denoising DAE provided superior performance on the three objective evaluations.},
	language = {en},
	author = {Lu, Xugang and Tsao, Yu and Matsuda, Shigeki and Hori, Chiori},
	pages = {5},
	file = {Lu et al. - Speech Enhancement Based on Deep Denoising Autoenc.pdf:/Users/miccio/Zotero/storage/Z7WCLIPF/Lu et al. - Speech Enhancement Based on Deep Denoising Autoenc.pdf:application/pdf}
}


@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-04-17},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/4PX3AQDD/1609.html:text/html;Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:/Users/miccio/Zotero/storage/REGCM3XP/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf}
}