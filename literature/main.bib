
@article{kolbaek_single-microphone_2018,
	title = {Single-{Microphone} {Speech} {Enhancement} and {Separation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1808.10620},
	abstract = {The cocktail party problem comprises the challenging task of understanding a speech signal in a complex acoustic environment, where multiple speakers and background noise signals simultaneously interfere with the speech signal of interest. A signal processing algorithm that can effectively increase the speech intelligibility and quality of speech signals in such complicated acoustic situations is highly desirable. Especially for applications involving mobile communication devices and hearing assistive devices. Due to the re-emergence of machine learning techniques, today, known as deep learning, the challenges involved with such algorithms might be overcome. In this PhD thesis, we study and develop deep learning-based techniques for two sub-disciplines of the cocktail party problem: single-microphone speech enhancement and single-microphone multi-talker speech separation. Specifically, we conduct in-depth empirical analysis of the generalizability capability of modern deep learning-based single-microphone speech enhancement algorithms. We show that performance of such algorithms is closely linked to the training data, and good generalizability can be achieved with carefully designed training data. Furthermore, we propose uPIT, a deep learning-based algorithm for single-microphone speech separation and we report state-of-the-art results on a speaker-independent multi-talker speech separation task. Additionally, we show that uPIT works well for joint speech separation and enhancement without explicit prior knowledge about the noise type or number of speakers. Finally, we show that deep learning-based speech enhancement algorithms designed to minimize the classical short-time spectral amplitude mean squared error leads to enhanced speech signals which are essentially optimal in terms of STOI, a state-of-the-art speech intelligibility estimator.},
	urldate = {2019-04-17},
	journal = {arXiv:1808.10620 [cs, eess]},
	author = {Kolbæk, Morten},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10620},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: PhD Thesis. 233 pages},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/7GZRYR96/1808.html:text/html;Kolbæk - 2018 - Single-Microphone Speech Enhancement and Separatio.pdf:/Users/miccio/Zotero/storage/4245V375/Kolbæk - 2018 - Single-Microphone Speech Enhancement and Separatio.pdf:application/pdf}
}

@article{wang_supervised_2017,
	title = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}: {An} {Overview}},
	shorttitle = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}},
	url = {http://arxiv.org/abs/1708.07524},
	abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
	urldate = {2019-04-17},
	journal = {arXiv:1708.07524 [cs]},
	author = {Wang, DeLiang and Chen, Jitong},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07524},
	keywords = {Computer Science - Sound, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 27 pages, 17 figures},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/PLIFEXMS/1708.html:text/html;Wang and Chen - 2017 - Supervised Speech Separation Based on Deep Learnin.pdf:/Users/miccio/Zotero/storage/ADSTPLFF/Wang and Chen - 2017 - Supervised Speech Separation Based on Deep Learnin.pdf:application/pdf}
}

@article{fu_complex_2017,
	title = {Complex spectrogram enhancement by convolutional neural network with multi-metrics learning},
	url = {http://arxiv.org/abs/1704.08504},
	abstract = {This paper aims to address two issues existing in the current speech enhancement methods: 1) the difficulty of phase estimations; 2) a single objective function cannot consider multiple metrics simultaneously. To solve the first problem, we propose a novel convolutional neural network (CNN) model for complex spectrogram enhancement, namely estimating clean real and imaginary (RI) spectrograms from noisy ones. The reconstructed RI spectrograms are directly used to synthesize enhanced speech waveforms. In addition, since log-power spectrogram (LPS) can be represented as a function of RI spectrograms, its reconstruction is also considered as another target. Thus a unified objective function, which combines these two targets (reconstruction of RI spectrograms and LPS), is equivalent to simultaneously optimizing two commonly used objective metrics: segmental signal-to-noise ratio (SSNR) and logspectral distortion (LSD). Therefore, the learning process is called multi-metrics learning (MML). Experimental results confirm the effectiveness of the proposed CNN with RI spectrograms and MML in terms of improved standardized evaluation metrics on a speech enhancement task.},
	urldate = {2019-04-17},
	journal = {arXiv:1704.08504 [cs, stat]},
	author = {Fu, Szu-Wei and Hu, Ting-yao and Tsao, Yu and Lu, Xugang},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08504},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/CRTJWEPL/1704.html:text/html;Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:/Users/miccio/Zotero/storage/78N8IRC7/Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:application/pdf}
}

@article{roux_phasebook_2018,
	title = {Phasebook and {Friends}: {Leveraging} {Discrete} {Representations} for {Source} {Separation}},
	shorttitle = {Phasebook and {Friends}},
	url = {http://arxiv.org/abs/1810.01395},
	abstract = {Deep learning based speech enhancement and source separation systems have recently reached unprecedented levels of quality, to the point that performance is reaching a new ceiling. Most systems rely on estimating the magnitude of a target source by estimating a real-valued mask to be applied to a time-frequency representation of the mixture signal. A limiting factor in such approaches is a lack of phase estimation: the phase of the mixture is most often used when reconstructing the estimated time-domain signal. Here, we propose "magbook", "phasebook", and "combook", three new types of layers based on discrete representations that can be used to estimate complex time-frequency masks. Magbook layers extend classical sigmoidal units and a recently introduced convex softmax activation for mask-based magnitude estimation. Phasebook layers use a similar structure to give an estimate of the phase mask without suffering from phase wrapping issues. Combook layers are an alternative to the magbook-phasebook combination that directly estimate complex masks. We present various training and inference schemes involving these representations, and explain in particular how to include them in an end-to-end learning framework. We also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations. We evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus for single-channel speaker-independent speaker separation, matching the performance of state-of-the-art mask-based approaches without requiring additional phase reconstruction steps.},
	urldate = {2019-04-17},
	journal = {arXiv:1810.01395 [cs, eess, stat]},
	author = {Roux, Jonathan Le and Wichern, Gordon and Watanabe, Shinji and Sarroff, Andy and Hershey, John R.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01395},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1810.01395 PDF:/Users/miccio/Zotero/storage/4KCJKJ4E/Roux et al. - 2018 - Phasebook and Friends Leveraging Discrete Represe.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/45NPB657/1810.html:text/html}
}

@article{zhen_psychoacoustically_2018,
	title = {On {Psychoacoustically} {Weighted} {Cost} {Functions} {Towards} {Resource}-{Efficient} {Deep} {Neural} {Networks} for {Speech} {Denoising}},
	url = {http://arxiv.org/abs/1801.09774},
	abstract = {We present a psychoacoustically enhanced cost function to balance network complexity and perceptual performance of deep neural networks for speech denoising. While training the network, we utilize perceptual weights added to the ordinary mean-squared error to emphasize contribution from frequency bins which are most audible while ignoring error from inaudible bins. To generate the weights, we employ psychoacoustic models to compute the global masking threshold from the clean speech spectra. We then evaluate the speech denoising performance of our perceptually guided neural network by using both objective and perceptual sound quality metrics, testing on various network structures ranging from shallow and narrow ones to deep and wide ones. The experimental results showcase our method as a valid approach for infusing perceptual significance to deep neural network operations. In particular, the more perceptually sensible enhancement in performance seen by simple neural network topologies proves that the proposed method can lead to resource-efficient speech denoising implementations in small devices without degrading the perceived signal fidelity.},
	urldate = {2019-04-17},
	journal = {arXiv:1801.09774 [cs, eess]},
	author = {Zhen, Kai and Sivaraman, Aswin and Sung, Jongmo and Kim, Minje},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.09774},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 5 pages, 4 figures},
	file = {arXiv\:1801.09774 PDF:/Users/miccio/Zotero/storage/BBBF8AZM/Zhen et al. - 2018 - On Psychoacoustically Weighted Cost Functions Towa.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/SB2GTY87/1801.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-04-17},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.03167 PDF:/Users/miccio/Zotero/storage/5UQ44P38/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/9PVBPIRY/1502.html:text/html}
}

@inproceedings{pons_experimenting_2016,
	title = {Experimenting with musically motivated convolutional neural networks},
	doi = {10.1109/CBMI.2016.7500246},
	abstract = {A common criticism of deep learning relates to the difficulty in understanding the underlying relationships that the neural networks are learning, thus behaving like a black-box. In this article we explore various architectural choices of relevance for music signals classification tasks in order to start understanding what the chosen networks are learning. We first discuss how convolutional filters with different shapes can fit specific musical concepts and based on that we propose several musically motivated architectures. These architectures are then assessed by measuring the accuracy of the deep learning model in the prediction of various music classes using a known dataset of audio recordings of ballroom music. The classes in this dataset have a strong correlation with tempo, what allows assessing if the proposed architectures are learning frequency and/or time dependencies. Additionally, a black-box model is proposed as a baseline for comparison. With these experiments we have been able to understand what some deep learning based algorithms can learn from a particular set of data.},
	booktitle = {2016 14th {International} {Workshop} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	author = {Pons, J. and Lidy, T. and Serra, X.},
	month = jun,
	year = {2016},
	keywords = {Neural networks, audio recording dataset, audio signal processing, audio tempo, ballroom music, black-box model, Computer architecture, convolutional filters, deep learning, filtering theory, Instruments, learning (artificial intelligence), learning frequency dependencies, learning time dependencies, Machine learning, music, Music, music class prediction, music signal classification tasks, musical concepts, musically-motivated convolutional neural network architecture, neural net architecture, Shape, signal classification, Time-frequency analysis},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/miccio/Zotero/storage/LW7UHMMG/7500246.html:text/html;Pons et al. - 2016 - Experimenting with musically motivated convolution.pdf:/Users/miccio/Zotero/storage/CENBI2GI/Pons et al. - 2016 - Experimenting with musically motivated convolution.pdf:application/pdf}
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2019-04-17},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1803.01271 PDF:/Users/miccio/Zotero/storage/MNJ37Q67/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/H2Y2CBVI/1803.html:text/html}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-04-17},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/miccio/Zotero/storage/4PX3AQDD/1609.html:text/html;Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:/Users/miccio/Zotero/storage/REGCM3XP/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf}
}

@inproceedings{marchi_novel_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional {LSTM} neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178320/},
	doi = {10.1109/ICASSP.2015.7178320},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Marchi, Erik and Vesperini, Fabio and Eyben, Florian and Squartini, Stefano and Schuller, Bjorn},
	month = apr,
	year = {2015},
	pages = {1996--2000},
	file = {Marchi et al. - 2015 - A novel approach for automatic acoustic novelty de.pdf:/Users/miccio/Zotero/storage/NHP466KK/Marchi et al. - 2015 - A novel approach for automatic acoustic novelty de.pdf:application/pdf}
}

@inproceedings{weninger_discriminatively_2014,
	address = {Atlanta, GA, USA},
	title = {Discriminatively trained recurrent neural networks for single-channel speech separation},
	isbn = {978-1-4799-7088-9},
	url = {http://ieeexplore.ieee.org/document/7032183/},
	doi = {10.1109/GlobalSIP.2014.7032183},
	abstract = {This paper describes an in-depth investigation of training criteria, network architectures and feature representations for regressionbased single-channel speech separation with deep neural networks (DNNs). We use a generic discriminative training criterion corresponding to optimal source reconstruction from time-frequency masks, and introduce its application to speech separation in a reduced feature space (Mel domain). A comparative evaluation of time-frequency mask estimation by DNNs, recurrent DNNs and non-negative matrix factorization on the 2nd CHiME Speech Separation and Recognition Challenge shows consistent improvements by discriminative training, whereas long short-term memory recurrent DNNs obtain the overall best results. Furthermore, our results conﬁrm the importance of ﬁne-tuning the feature representation for DNN training.},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2014 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing} ({GlobalSIP})},
	publisher = {IEEE},
	author = {Weninger, Felix and Hershey, John R. and Le Roux, Jonathan and Schuller, Bjorn},
	month = dec,
	year = {2014},
	pages = {577--581},
	file = {Weninger et al. - 2014 - Discriminatively trained recurrent neural networks.pdf:/Users/miccio/Zotero/storage/U62EJZ9J/Weninger et al. - 2014 - Discriminatively trained recurrent neural networks.pdf:application/pdf}
}

@inproceedings{erdogan_phase-sensitive_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178061/},
	doi = {10.1109/ICASSP.2015.7178061},
	abstract = {Separation of speech embedded in non-stationary interference is a challenging problem that has recently seen dramatic improvements using deep network-based methods. Previous work has shown that estimating a masking function to be applied to the noisy spectrum is a viable approach that can be improved by using a signalapproximation based objective function. Better modeling of dynamics through deep recurrent networks has also been shown to improve performance. Here we pursue both of these directions. We develop a phase-sensitive objective function based on the signal-to-noise ratio (SNR) of the reconstructed signal, and show that in experiments it yields uniformly better results in terms of signal-to-distortion ratio (SDR). We also investigate improvements to the modeling of dynamics, using bidirectional recurrent networks, as well as by incorporating speech recognition outputs in the form of alignment vectors concatenated with the spectral input features. Both methods yield further improvements, pointing to tighter integration of recognition with separation as a promising future direction.},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Erdogan, Hakan and Hershey, John R. and Watanabe, Shinji and Le Roux, Jonathan},
	month = apr,
	year = {2015},
	pages = {708--712},
	file = {Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:/Users/miccio/Zotero/storage/XS3WKTAN/Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:application/pdf}
}

@article{grais_single_2017,
	title = {Single {Channel} {Audio} {Source} {Separation} using {Convolutional} {Denoising} {Autoencoders}},
	url = {http://arxiv.org/abs/1703.08019},
	abstract = {Deep learning techniques have been used recently to tackle the audio source separation problem. In this work, we propose to use deep fully convolutional denoising autoencoders (CDAEs) for monaural audio source separation. We use as many CDAEs as the number of sources to be separated from the mixed signal. Each CDAE is trained to separate one source and treats the other sources as background noise. The main idea is to allow each CDAE to learn suitable spectral-temporal ﬁlters and features to its corresponding source. Our experimental results show that CDAEs perform source separation slightly better than the deep feedforward neural networks (FNNs) even with fewer parameters than FNNs.},
	language = {en},
	urldate = {2019-04-17},
	journal = {arXiv:1703.08019 [cs]},
	author = {Grais, Emad M. and Plumbley, Mark D.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.08019},
	keywords = {Computer Science - Sound, 68T01, H.5.5, I.2.6, I.4.3, I.5},
	annote = {Comment: Accepted at GlobalSIP 2017 and the final version is available at http://epubs.surrey.ac.uk/841860/},
	file = {Grais and Plumbley - 2017 - Single Channel Audio Source Separation using Convo.pdf:/Users/miccio/Zotero/storage/G38N9294/Grais and Plumbley - 2017 - Single Channel Audio Source Separation using Convo.pdf:application/pdf}
}

@article{kayser_denoising_nodate,
	title = {Denoising {Convolutional} {Autoencoders} for {Noisy} {Speech} {Recognition}},
	abstract = {We propose the use of a deep denoising convolutional autoencoder to mitigate problems of noise in real-world automatic speech recognition. We propose an overall pipeline for denoising, experiment with a variety of network architectures and conﬁguration parameters, and report results on an intermediate reconstruction error metric. Our experiments show that optimal conﬁguration for convolutional denoising varies signiﬁcantly from that for standard CNN tasks such as image recognition. Further, the proposed method easily beats a simple baseline and anecdotally displays interesting behavior. A number of avenues for further research are discussed.},
	language = {en},
	author = {Kayser, Mike and Zhong, Victor},
	pages = {6},
	file = {Kayser and Zhong - Denoising Convolutional Autoencoders for Noisy Spe.pdf:/Users/miccio/Zotero/storage/JYM3DZH5/Kayser and Zhong - Denoising Convolutional Autoencoders for Noisy Spe.pdf:application/pdf}
}

@article{lu_speech_nodate,
	title = {Speech {Enhancement} {Based} on {Deep} {Denoising} {Autoencoder}},
	abstract = {We previously have applied deep autoencoder (DAE) for noise reduction and speech enhancement. However, the DAE was trained using only clean speech. In this study, by using noisyclean training pairs, we further introduce a denoising process in learning the DAE. In training the DAE, we still adopt greedy layer-wised pretraining plus ﬁne tuning strategy. In pretraining, each layer is trained as a one-hidden-layer neural autoencoder (AE) using noisy-clean speech pairs as input and output (or transformed noisy-clean speech pairs by preceding AEs). Fine tuning was done by stacking all AEs with pretrained parameters for initialization. The trained DAE is used as a ﬁlter for speech estimation when noisy speech is given. Speech enhancement experiments were done to examine the performance of the trained denoising DAE. Noise reduction, speech distortion, and perceptual evaluation of speech quality (PESQ) criteria are used in the performance evaluations. Experimental results show that adding depth of the DAE consistently increase the performance when a large training data set is given. In addition, compared with a minimum mean square error based speech enhancement algorithm, our proposed denoising DAE provided superior performance on the three objective evaluations.},
	language = {en},
	author = {Lu, Xugang and Tsao, Yu and Matsuda, Shigeki and Hori, Chiori},
	pages = {5},
	file = {Lu et al. - Speech Enhancement Based on Deep Denoising Autoenc.pdf:/Users/miccio/Zotero/storage/Z7WCLIPF/Lu et al. - Speech Enhancement Based on Deep Denoising Autoenc.pdf:application/pdf}
}

@book{smith_mathematics_2003,
	address = {Place of publication not identified]; Stanford, Calif.},
	title = {Mathematics of the discrete {Fourier} transform ({DFT}): with music and audio applicaitons},
	isbn = {978-0-9745607-0-0},
	shorttitle = {Mathematics of the discrete {Fourier} transform ({DFT})},
	language = {English},
	publisher = {W3K Pub. ; Center for Computer Research in Music and Acoustics},
	author = {Smith, Julius O},
	year = {2003},
	note = {OCLC: 60379068}
}

@article{han_learning_2015,
	title = {Learning {Spectral} {Mapping} for {Speech} {Dereverberation} and {Denoising}},
	volume = {23},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2015.2416653},
	abstract = {In real-world environments, human speech is usually distorted by both reverberation and background noise, which have negative effects on speech intelligibility and speech quality. They also cause performance degradation in many speech technology applications, such as automatic speech recognition. Therefore, the dereverberation and denoising problems must be dealt with in daily listening environments. In this paper, we propose to perform speech dereverberation using supervised learning, and the supervised approach is then extended to address both dereverberation and denoising. Deep neural networks are trained to directly learn a spectral mapping from the magnitude spectrogram of corrupted speech to that of clean speech. The proposed approach substantially attenuates the distortion caused by reverberation, as well as background noise, and is conceptually simple. Systematic experiments show that the proposed approach leads to significant improvements of predicted speech intelligibility and quality, as well as automatic speech recognition in reverberant noisy conditions. Comparisons show that our approach substantially outperforms related methods.},
	number = {6},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Han, K. and Wang, Y. and Wang, D. and Woods, W. S. and Merks, I. and Zhang, T.},
	month = jun,
	year = {2015},
	keywords = {learning (artificial intelligence), automatic speech recognition, background noise, corrupted speech, deep neural networks, Deep neural networks (DNNs), denoising, dereverberation, human speech, magnitude spectrogram, neural nets, Noise reduction, real-world environments, reverberant noisy conditions, reverberation, Reverberation, reverberation noise, spectral mapping, Spectrogram, Speech, speech denoising, speech dereverberation, speech intelligibility, Speech processing, speech quality, speech recognition, supervised learning, Time-domain analysis, Training},
	pages = {982--992},
	file = {IEEE Xplore Abstract Record:/Users/miccio/Zotero/storage/EHBRD96B/7067387.html:text/html}
}

@inproceedings{christensen_chime_2010,
	title = {The {CHiME} corpus: a resource and a challenge for computational hearing in multisource environments},
	shorttitle = {The {CHiME} corpus},
	abstract = {We present a new corpus designed for noise-robust speech processing research, CHiME. Our goal was to produce material which is both natural (derived from reverberant domestic environments with many simultaneous and unpredictable sound sources) and controlled (providing an enumerated range of SNRs spanning 20 dB). The corpus includes around 40 hours of background recordings from a head and torso simulator positioned in a domestic setting, and a comprehensive set of binaural impulse responses collected in the same environment. These have been used to add target utterances from the Grid speech recognition corpus into the CHiME domestic setting. Data has been mixed in a manner that produces a controlled and yet natural range of SNRs over which speech separation, enhancement and recognition algorithms can be evaluated. The paper motivates the design of the corpus, and describes the collection and post-processing of the data. We also present a set of baseline recognition results.},
	booktitle = {{INTERSPEECH}},
	author = {Christensen, Heidi and Barker, Jon and Ma, Ning and Green, Phil D.},
	year = {2010},
	keywords = {Speech processing, Algorithm, Baseline (configuration management), Binaural beats, File spanning, Simulation, Speech recognition, Text corpus, Video post-processing},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/D3D9KSEG/Christensen et al. - 2010 - The CHiME corpus a resource and a challenge for c.pdf:application/pdf}
}

@article{vincent_performance_2006,
	title = {Performance measurement in blind audio source separation},
	volume = {14},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/1643671/},
	doi = {10.1109/TSA.2005.858005},
	number = {4},
	urldate = {2019-04-17},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Vincent, E. and Gribonval, R. and Fevotte, C.},
	month = jul,
	year = {2006},
	pages = {1462--1469},
	file = {Vincent et al. - 2006 - Performance measurement in blind audio source sepa.pdf:/Users/miccio/Zotero/storage/335YVH4S/Vincent et al. - 2006 - Performance measurement in blind audio source sepa.pdf:application/pdf}
}

@inproceedings{taal_short-time_2010,
	address = {Dallas, TX, USA},
	title = {A short-time objective intelligibility measure for time-frequency weighted noisy speech},
	isbn = {978-1-4244-4295-9},
	url = {http://ieeexplore.ieee.org/document/5495701/},
	doi = {10.1109/ICASSP.2010.5495701},
	urldate = {2019-04-17},
	booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	year = {2010},
	pages = {4214--4217}
}

@article{taal_algorithm_2011,
	title = {An {Algorithm} for {Intelligibility} {Prediction} of {Time}–{Frequency} {Weighted} {Noisy} {Speech}},
	volume = {19},
	issn = {1558-7916},
	doi = {10.1109/TASL.2011.2114881},
	abstract = {In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.},
	number = {7},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Taal, C. H. and Hendriks, R. C. and Heusdens, R. and Jensen, J.},
	month = sep,
	year = {2011},
	keywords = {Noise reduction, Speech, speech intelligibility, Speech processing, Correlation, intelligibility prediction, Noise measurement, noise-reduction algorithm, noisy unprocessed speech, objective intelligibility model, objective machine-driven intelligibility measure, objective measure, short-time objective intelligibility measure, Signal to noise ratio, speech enhancement, speech intelligibility prediction, Time frequency analysis, time-frequency weighted noisy speech},
	pages = {2125--2136},
	file = {IEEE Xplore Abstract Record:/Users/miccio/Zotero/storage/RUQGBE3T/5713237.html:text/html}
}

@article{hsu_learning_2017,
	title = {Learning {Latent} {Representations} for {Speech} {Generation} and {Transformation}},
	url = {http://arxiv.org/abs/1704.04222},
	abstract = {An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.},
	urldate = {2019-04-17},
	journal = {arXiv:1704.04222 [cs, stat]},
	author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04222},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to Interspeech 2017},
	file = {arXiv\:1704.04222 PDF:/Users/miccio/Zotero/storage/AWUI68XV/Hsu et al. - 2017 - Learning Latent Representations for Speech Generat.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/A8NSQEY4/1704.html:text/html}
}

@inproceedings{mcfee_librosa:_2015,
	title = {librosa: {Audio} and {Music} {Signal} {Analysis} in {Python}},
	shorttitle = {librosa},
	doi = {10.25080/majora-7b98e3ed-003},
	abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library’s functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
	author = {McFee, Brian and Raffel, Colin A. and Liang, Dawen and Ellis, Daniel Patrick Whittlesey and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	year = {2015},
	keywords = {Audio Media, explanation, High-level programming language, Information retrieval, Python, Signal processing, Software development},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/P6B9D84F/McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:application/pdf}
}

@article{oliphant_python_2007,
	title = {Python for {Scientific} {Computing}},
	volume = {9},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4160250/},
	doi = {10.1109/MCSE.2007.58},
	number = {3},
	urldate = {2019-04-17},
	journal = {Computing in Science \& Engineering},
	author = {Oliphant, Travis E.},
	year = {2007},
	pages = {10--20},
	file = {Submitted Version:/Users/miccio/Zotero/storage/GIGVB574/Oliphant - 2007 - Python for Scientific Computing.pdf:application/pdf}
}

@article{zhao_convolutional-recurrent_2018,
	title = {Convolutional-{Recurrent} {Neural} {Networks} for {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/1805.00579},
	abstract = {We propose an end-to-end model based on convolutional and recurrent neural networks for speech enhancement. Our model is purely data-driven and does not make any assumptions about the type or the stationarity of the noise. In contrast to existing methods that use multilayer perceptrons (MLPs), we employ both convolutional and recurrent neural network architectures. Thus, our approach allows us to exploit local structures in both the frequency and temporal domains. By incorporating prior knowledge of speech signals into the design of model structures, we build a model that is more data-efficient and achieves better generalization on both seen and unseen noise. Based on experiments with synthetic data, we demonstrate that our model outperforms existing methods, improving PESQ by up to 0.6 on seen noise and 0.64 on unseen noise.},
	urldate = {2019-04-17},
	journal = {arXiv:1805.00579 [cs, eess]},
	author = {Zhao, Han and Zarar, Shuayb and Tashev, Ivan and Lee, Chin-Hui},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00579},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: ICASSP 2018},
	file = {arXiv\:1805.00579 PDF:/Users/miccio/Zotero/storage/APBX98ZN/Zhao et al. - 2018 - Convolutional-Recurrent Neural Networks for Speech.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/9XCXWWWU/1805.html:text/html}
}

@inproceedings{fakoor_constrained_2018,
	address = {Calgary, AB},
	title = {Constrained {Convolutional}-{Recurrent} {Networks} to {Improve} {Speech} {Quality} with {Low} {Impact} on {Recognition} {Accuracy}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462042/},
	doi = {10.1109/ICASSP.2018.8462042},
	abstract = {For a speech-enhancement algorithm, it is highly desirable to simultaneously improve perceptual quality and recognition rate. Thanks to limitation on the cost functions, it is challenging to train a model that effectively optimizes both metrics at the same time. In this paper, we propose a method for speech enhancement that combines local and global contextual structures information through convolutional-recurrent neural networks that improves perceptual quality. At the same time, we introduce a new constraint on the objective function using a language model/decoder that limits the impact on recognition rate. Based on experiments conducted with real user data, we demonstrate that our new context-augmented machinelearning approach for speech enhancement improves PESQ and WER by an additional 24.5\% and 51.3\%, respectively, when compared to the best-performing methods in the literature.},
	language = {en},
	urldate = {2019-04-17},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Fakoor, Rasool and He, Xiaodong and Tashev, Ivan and Zarar, Shuayb},
	month = apr,
	year = {2018},
	pages = {3011--3015},
	file = {Fakoor et al. - 2018 - Constrained Convolutional-Recurrent Networks to Im.pdf:/Users/miccio/Zotero/storage/6N8ZMCXI/Fakoor et al. - 2018 - Constrained Convolutional-Recurrent Networks to Im.pdf:application/pdf}
}

@article{tu_hybrid_2018,
	title = {A {Hybrid} {Approach} to {Combining} {Conventional} and {Deep} {Learning} {Techniques} for {Single}-channel {Speech} {Enhancement} and {Recognition}},
	url = {https://www.microsoft.com/en-us/research/publication/a-hybrid-approach-to-combining-conventional-and-deep-learning-techniques-for-single-channel-speech-enhancement-and-recognition/},
	abstract = {Conventional speech-enhancement techniques employ statistical signal-processing algorithms. They are computationally efficient and improve speech quality even under unknown noise conditions. For these reasons, they are preferred for deployment in unpredictable environments. One limitation of these algorithms is that they fail to suppress non-stationary noise. This hinders their broad usage. Emerging algorithms based on deep-learning promise …},
	language = {en-US},
	urldate = {2019-04-17},
	author = {Tu, Yan-Hui and Tashev, Ivan and Zarar, Shuayb and Lee, Chin-Hui},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/59BS48MG/Tu et al. - 2018 - A Hybrid Approach to Combining Conventional and De.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/4ERKBB2J/a-hybrid-approach-to-combining-conventional-and-deep-learning-techniques-for-single-channel-spe.html:text/html}
}

@article{srivastava_understanding_2014,
	title = {Understanding {Locally} {Competitive} {Networks}},
	url = {http://arxiv.org/abs/1410.1165},
	abstract = {Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of computational units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks.},
	urldate = {2019-04-17},
	journal = {arXiv:1410.1165 [cs]},
	author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Jürgen},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.1165},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, 68T30, 68T10},
	annote = {Comment: 9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track},
	file = {arXiv\:1410.1165 PDF:/Users/miccio/Zotero/storage/LPHNAFQ8/Srivastava et al. - 2014 - Understanding Locally Competitive Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/PGD47WTN/1410.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	urldate = {2019-04-17},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/3Q2V7GZG/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/VWXX8796/srivastava14a.html:text/html}
}

@article{ko_limiting_2018,
	title = {Limiting {Numerical} {Precision} of {Neural} {Networks} to {Achieve} {Real}-time {Voice} {Activity} {Detection}},
	url = {https://www.microsoft.com/en-us/research/publication/limiting-numerical-precision-of-neural-networks-to-achieve-real-time-voice-activity-detection/},
	abstract = {Fast and robust voice-activity detection is critical to efficiently process speech. While deep-learning based methods to detect voice have shown competitive accuracy, the best models in the literature incur over a 100 ms latency on commodity processors. Such delays are unacceptable for real-time speech processing. In this paper, we study the impact of lowering the …},
	language = {en-US},
	urldate = {2019-04-17},
	author = {Ko, Jong-Hwan and Fromm, Josh and Philipose, Matthai and Tashev, Ivan and Zarar, Shuayb},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/9J5DHV89/Ko et al. - 2018 - Limiting Numerical Precision of Neural Networks to.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/XHKKHHSU/limiting-numerical-precision-of-neural-networks-to-achieve-real-time-voice-activity-detection.html:text/html}
}

@article{park_fully_2016,
	title = {A {Fully} {Convolutional} {Neural} {Network} for {Speech} {Enhancement}},
	url = {https://arxiv.org/abs/1609.07132v1},
	abstract = {In hearing aids, the presence of babble noise degrades hearing
intelligibility of human speech greatly. However, removing the babble without
creating artifacts in human speech is a challenging task in a low SNR
environment. Here, we sought to solve the problem by finding a `mapping'
between noisy speech spectra and clean speech spectra via supervised learning.
Specifically, we propose using fully Convolutional Neural Networks, which
consist of lesser number of parameters than fully connected networks. The
proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates
that a convolutional network can be 12 times smaller than a recurrent network
and yet achieves better performance, which shows its applicability for an
embedded system: the hearing aids.},
	language = {en},
	urldate = {2019-04-20},
	author = {Park, Se Rim and Lee, Jinwon},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/RNMADI9Z/Park and Lee - 2016 - A Fully Convolutional Neural Network for Speech En.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/H7HHAD2S/1609.html:text/html}
}

@misc{thiemann_demand:_2013,
	title = {Demand: {A} {Collection} {Of} {Multi}-{Channel} {Recordings} {Of} {Acoustic} {Noise} {In} {Diverse} {Environments}},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	shorttitle = {Demand},
	url = {https://zenodo.org/record/1227121},
	abstract = {{\textless}strong{\textgreater}DEMAND: Diverse Environments Multichannel Acoustic Noise Database{\textless}/strong{\textgreater}

A database of 16-channel environmental noise recordings

{\textless}strong{\textgreater}Introduction{\textless}/strong{\textgreater}

Microphone arrays, a (typically regular) arrangement of several microphones, allow for a number of interesting signal processing techniques. The correlation of audio signals from microphones that are located in close proximity with each other can, for example, be used to determine the spatial location of sound source relative to the array, or to isolate or enhance a signal based on the direction from which the sound reaches the array.

Typically, experiments with microphone arrays that consider acoustic background noise use controlled environments or simulated environments. Such artificial setups will in general be sparse in terms of noise sources. Other pre-existing real-world noise databases (e.g. the AURORA-2 corpus, the CHiME background noise data, or the NOISEX-92 database) tend to provide only a very limited variety of environments and are limited to at most 2 channels.

The DEMAND (Diverse Environments Multichannel Acoustic Noise Database) presented here provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.

{\textless}strong{\textgreater}License{\textless}/strong{\textgreater}

This work, the audio data and the document describing it, is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.

{\textless}strong{\textgreater}The data{\textless}/strong{\textgreater}

A description of the data and the recording equipment is provided in the file {\textless}strong{\textgreater}DEMAND.pdf{\textless}/strong{\textgreater}. All recordings are available as 16 single-channel WAV files in one directory at both 48 kHz and 16 kHz sampling rates. All files are compressed into "zip" files.

{\textless}strong{\textgreater}Other information{\textless}/strong{\textgreater}

The MATLAB scripts listed in the documentation can be found in the file {\textless}strong{\textgreater}scripts.zip{\textless}/strong{\textgreater}.

{\textless}strong{\textgreater}The Authors{\textless}/strong{\textgreater}

This work was created by Joachim Thiemann (IRISA-CNRS), Nobutaka Ito (University of Tokyo), and Emmanuel Vincent (Inria Rennes - Bretagne Atlantique). It was supported by Inria under the Associate Team Program VERSAMUS.},
	language = {en},
	urldate = {2019-04-22},
	publisher = {Zenodo},
	author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
	month = jun,
	year = {2013},
	doi = {10.5281/zenodo.1227121},
	note = {type: dataset},
	keywords = {Microphone Array, Multichannel Audio, Noise},
	annote = {Other
Supported by Inria under the Associate Team Program VERSAMUS}
}

@article{chollet_xception:_2016,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2019-04-25},
	journal = {arXiv:1610.02357 [cs]},
	author = {Chollet, François},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1610.02357 PDF:/Users/miccio/Zotero/storage/A3RV4IPL/Chollet - 2016 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/2EVGG57Q/1610.html:text/html}
}

@article{cherry_experiments_1953,
	title = {Some {Experiments} on the {Recognition} of {Speech}, with {One} and with {Two} {Ears}},
	volume = {25},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/1.1907229},
	doi = {10.1121/1.1907229},
	number = {5},
	urldate = {2019-04-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cherry, E. Colin},
	month = sep,
	year = {1953},
	pages = {975--979},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/H9VEL75D/Cherry - 1953 - Some Experiments on the Recognition of Speech, wit.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/FQRF6P3H/1.html:text/html}
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2019-04-26},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Approximation, Completeness, Neural networks},
	pages = {303--314},
	file = {Springer Full Text PDF:/Users/miccio/Zotero/storage/65C8N6GF/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf}
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	number = {2},
	urldate = {2019-04-26},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	month = jan,
	year = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
	file = {ScienceDirect Full Text PDF:/Users/miccio/Zotero/storage/RDUYUGY5/Hornik - 1991 - Approximation capabilities of multilayer feedforwa.pdf:application/pdf;ScienceDirect Snapshot:/Users/miccio/Zotero/storage/BEUNNZ4Y/089360809190009T.html:text/html}
}

@article{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2019-04-27},
	journal = {arXiv:1704.01279 [cs]},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1704.01279 PDF:/Users/miccio/Zotero/storage/WXGTYPUM/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/Users/miccio/Zotero/storage/KPYDKNQ4/1704.html:text/html}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {60},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	number = {6},
	urldate = {2019-04-28},
	journal = {Commun. ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {ACM Full Text PDF:/Users/miccio/Zotero/storage/YX2HQAU5/Krizhevsky et al. - 2017 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}

@article{glorot_deep_nodate,
	title = {Deep {Sparse} {Rectiﬁer} {Neural} {Networks}},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-diﬀerentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectiﬁer networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the diﬃculty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	pages = {9},
	file = {Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf:/Users/miccio/Zotero/storage/W9IATHBG/Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf:application/pdf}
}

@misc{karpathy_cs231n_nodate,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/neural-networks-1/},
	urldate = {2019-04-28},
	author = {Karpathy, Andrej},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/Users/miccio/Zotero/storage/MWA4S4F6/neural-networks-1.html:text/html}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {https://arxiv.org/abs/1609.04747v2},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are
often used as black-box optimizers, as practical explanations of their
strengths and weaknesses are hard to come by. This article aims to provide the
reader with intuitions with regard to the behaviour of different algorithms
that will allow her to put them to use. In the course of this overview, we look
at different variants of gradient descent, summarize challenges, introduce the
most common optimization algorithms, review architectures in a parallel and
distributed setting, and investigate additional strategies for optimizing
gradient descent.},
	language = {en},
	urldate = {2019-05-02},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/Users/miccio/Zotero/storage/BI6V3FPZ/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;Snapshot:/Users/miccio/Zotero/storage/G28U3PGT/1609.html:text/html}
}