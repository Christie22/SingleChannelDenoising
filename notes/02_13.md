# Notes from 13/02

Process:
1. Treat data
2. Replicate state of the art
3. Implement techniques
4. Design experiment
5. Apply techniques
6. Evaluate performances


Things to figure out:
- Datasets (which one to use, how, etc)
- State of the art (baseline)
- Metrics for evaluation
- Preprocessing
- ML techniques to try


State of the art:
- Short-Time Spectral Amplitude Minimum Mean Square Error (STSA-MMSE)
- Non-negative Matrix Factorization (NMF)

Architecture:
- Long Short-Term Memory Based Recurrent Neural Networks (LTSM)
- Permutation Invariant Training (PIT)
- Recurrent Neural Networks (RNN)
- (Subband Deep NN) [Wang & Wang 2012]
- Support Vector Machine (SVM) [Wang & Wang 2012]
-	Deep Autoencoder (DAE) [Lu et al. 2013]
- Restrictive Boltzmann Machine (RBM) for pretraining ("fine tuning") [Xu et al. 2013]
- Convolution NN (CNN) for temporal mapping [Fu et al.]
- Generative Adversarial Network (GAN)
- Speech Enhancement GAN (SEGAN) [Michelsanti & Tan, 2017]
- Reverberation-time-aware model [Wu et al]

Training targets: masking / mapping
- Ideal Binary Mask (IBM) 
- Ideal Ratio Mask (IRM) / complex IRM (cIRM)

Loss functions:
- Short-Time Spectral Amplitude Minimum Mean Square Error (STSA-MMSE)
- Cross-entropy

Features:
- Log-Power Spectrum (LPS)
- Spectrogram
- Multi-Resolution Cochleagram (MRCG)  [Han et al]
- Dynamic features (delta features and acceleration features) [	Xiao et al.]

Metrics (performance measures)
- Short-Time Objective Intelligibility (STOI) [55]
- Perceptual Evaluation of Speech Quality (PESQ) [56, 57]
- Signal to Distortion Ratio (SDR)
- Signal to Interferences Ratio (SIR)
- Signal to Artifact Ratio (SAR)[2, 58]
- Maximum Mean Envelope Linear Correlation (MMELC)


Refs:
- type [number]: See "Single-Microphone Speech Enhancement and Separation Using Deep Learning"
- type [name(s & date)]: See "Supervised Speech Separation Based on Deep Learning â€“ Overview"
