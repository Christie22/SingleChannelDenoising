# Let's train some models
> semester project group meeting 11/04

## Agenda
- Progresses (exp time: ~10)
  - `C` Implementation
    - model designer
    - flexible loss function with custom window
  - `R` Research
    - some audible results (hsu, lstm models)
    - no computational bottleneck on gpu
    - briefly experimented with loss funcs
    - 16-bit operations: not suitable for builtin batch norm
  - Other
    - got noise dataset from company
    - no progress on report

- `C` Roadmap (exp time: ~15)
  - Research
    - thoroughly test normalization!
    - investigate novel ML technologies (e.g. attention layers, TCN, etc)
    - test performances of models
  - Implementation
    - design relevant models
    - fix metrics/results scripts
    - improvements on framework (i.e. scripts parametrization)
    - real-time demo of some sort
  - Other
    - try real-world noise (fix noising functions first)
    - write report

- Challenges/Questions (exp time: til end of meeting)
  - `C` no exhaustive description of model architecture in the literature
  - `C` training plateaus after few generations
  - `C` normalization (0-mean, 1-std) on power spectrograms
  - `R` GPU memory maxes out easily
  - `R` lack of final validation dataset

## Notes
- properly keep track of settings for experiments: done
- apply a limited set of multiple noise variations: done, except real-world noise (wip)
- try:
  - polling (max, avg): not afaik
  - increase dropout (<0.5): done
- deal with plateaus
  - add layers: tried
  - smaller strides: tried (musically-informed conv)
  - increase dropout: tried
- fix results: done
- frame research question
- add references to bibtex


### models results
- 3x3 conv models: [vanilla, hsu-glass, dilation] x [3, 6, 12]
- 3 rnn models (2 recurrent layers):
  - fully-gru autoencoder
  - gru autoencoder w dense bottleneck
  - gru autoencoder w dense bottleneck and conv input outermost layers
- N TCN ???
- N attention layers ???